1. I was able to extract layer weights for images from the VGG CNN 16 layer pre-trained model up to the `pool5` layer. Upper layers are fully connected layers which aren't of any use (so far, at least) in the project. Now I can easily create a feature vector consisting of the weights of all layers for any image (see `test_vggcnn_16layer.ipynb`). It takes 05:37.598 to parse the model and layerwise transformation for a single image takes about 4.77 seconds. (I haven't yet tried batched transformation, I should see how much that takes agains the single image case)

2. I tested it on VGG CNN S network, and it is working fine (see `test_vggcnn_s_anothershot.ipynb`). It takes 03:19.25 to parse the model and layerwise transformation for a single image takes about 2.02 seconds.

3. I found out that by chopping the `fc` layers, the network becomes size independent. Thus I can feed in image if any size (granted the size at last layers doesn't shrink to zero). So this observation indicates that I need not think about resizing or cropping issues and I can now just focus on data augmentation.

4. Another attempt to use Reference CaffeNet model (a variant of AlexNet) was unsuccessful. 

5. Yasser Souri suggested Persian OCR using RNN as project. There are implementations of this project in the wild, but they work on Left-to-Right languages which have separate characters when written. I couldn't find previous work on Right-to-Left languages in which characters are connected when being written. I should think about it.

6. There also was a discussion on how to choose the feature vector for a single pixel. It is obvious that for better result, features from neigbor pixels should be used, too. In this case, features inside a cone should be used. Mr. Souri insisted that a cone with its base in the last layer should be used, though I think that instead its base should be on the first layer. Anyways I should try both ideas and see which works better.